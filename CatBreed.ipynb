{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'Abyssinian',\n 1: 'American Bobtail',\n 2: 'American Curl',\n 3: 'American Shorthair',\n 4: 'American Wirehair',\n 5: 'Applehead Siamese',\n 6: 'Balinese',\n 7: 'Bengal',\n 8: 'Birman',\n 9: 'Bombay',\n 10: 'British Shorthair',\n 11: 'Burmese',\n 12: 'Burmilla',\n 13: 'Calico',\n 14: 'Canadian Hairless',\n 15: 'Chartreux',\n 16: 'Chausie',\n 17: 'Chinchilla',\n 18: 'Cornish Rex',\n 19: 'Cymric',\n 20: 'Devon Rex',\n 21: 'Dilute Calico',\n 22: 'Dilute Tortoiseshell',\n 23: 'Domestic Long Hair',\n 24: 'Domestic Medium Hair',\n 25: 'Domestic Short Hair',\n 26: 'Egyptian Mau',\n 27: 'Exotic Shorthair',\n 28: 'Extra-Toes Cat - Hemingway Polydactyl',\n 29: 'Havana',\n 30: 'Himalayan',\n 31: 'Japanese Bobtail',\n 32: 'Javanese',\n 33: 'Korat',\n 34: 'LaPerm',\n 35: 'Maine Coon',\n 36: 'Manx',\n 37: 'Munchkin',\n 38: 'Nebelung',\n 39: 'Norwegian Forest Cat',\n 40: 'Ocicat',\n 41: 'Oriental Long Hair',\n 42: 'Oriental Short Hair',\n 43: 'Oriental Tabby',\n 44: 'Persian',\n 45: 'Pixiebob',\n 46: 'Ragamuffin',\n 47: 'Ragdoll',\n 48: 'Russian Blue',\n 49: 'Scottish Fold',\n 50: 'Selkirk Rex',\n 51: 'Siamese',\n 52: 'Siberian',\n 53: 'Silver',\n 54: 'Singapura',\n 55: 'Snowshoe',\n 56: 'Somali',\n 57: 'Sphynx - Hairless Cat',\n 58: 'Tabby',\n 59: 'Tiger',\n 60: 'Tonkinese',\n 61: 'Torbie',\n 62: 'Tortoiseshell',\n 63: 'Turkish Angora',\n 64: 'Turkish Van',\n 65: 'Tuxedo'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# breeds = os.listdir(\"CatBreed/images\")\n",
    "# breeds = sorted(breeds)\n",
    "# breeds.pop(0)\n",
    "# breed_map = dict()\n",
    "#\n",
    "# for i in range(len(breeds)):\n",
    "#     breed_map[i] = breeds[i]\n",
    "# breed_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('CatBreedMap.csv', 'w') as f:\n",
    "#     w = csv.writer(f)\n",
    "#     w.writerows(breed_map.items())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torchvision.datasets as datasets # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms # Transformations we can perform on our dataset\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu or cpu\n",
    "\n",
    "assert torch.backends.mps.is_available(), \"GPU is not available, check the directions above (or disable this assertion to use CPU)\"\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # use gpu or cpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FAST_RUN = False\n",
    "IMAGE_WIDTH=224\n",
    "IMAGE_HEIGHT=224\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3\n",
    "# DATA_PATH = \"/content/drive/Shareddrives/cse 455/train_data\"\n",
    "DATA_PATH = \"CatBreed\"\n",
    "BATCH_SIZE = 100\n",
    "SAVE_TO = \"Saved\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize((IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = datasets.ImageFolder(DATA_PATH, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data))\n",
    "test_size = int(0.1 * len(data))\n",
    "val_size = len(data) - train_size - test_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_size, val_size, test_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(data, [train_size, val_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "train_set,\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "val_set,\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "test_set,\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CatBreedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(3, 3), stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(3, 3), stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), stride=2)\n",
    "\n",
    "        # conected layers\n",
    "        self.fc1 = nn.Linear(in_features= 2304, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
    "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CatBreedCNN().to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 0.0015)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train():\n",
    "    EPOCHS = 1\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    # print_every = 100\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "\n",
    "      model.train()\n",
    "      train_loss = 0.0\n",
    "      train_acc = 0.0\n",
    "\n",
    "      # itr = 0\n",
    "      # start = time.time()\n",
    "\n",
    "      for (x_batch, labels) in train_loader:\n",
    "        x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        labels_pred = model(x_batch)\n",
    "        batch_loss = loss(labels_pred, labels)\n",
    "        train_loss = train_loss + batch_loss.item()\n",
    "\n",
    "        labels_pred_max = torch.argmax(labels_pred, 1)\n",
    "        batch_acc = torch.sum(labels_pred_max == labels)\n",
    "        train_acc = train_acc + batch_acc.item()\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(train_acc / (BATCH_SIZE * len(train_loader)))\n",
    "\n",
    "\n",
    "        # itr = itr + 1\n",
    "        # if iter % print_every == 0:\n",
    "        #       print_loss_avg = train_loss / print_every\n",
    "        #       print('%s (%d %d%%) %.4f' % (timeSince(start, iter / len(train_loader)),\n",
    "        #                                     iter, iter / len(train_loader) * 100, print_loss_avg))\n",
    "        # Validation loop; use .no_grad() context manager to save memory.\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "          for (v_batch, labels) in val_loader:\n",
    "              v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)\n",
    "              labels_pred = model(v_batch)\n",
    "              v_batch_loss = loss(labels_pred, labels)\n",
    "              val_loss = val_loss + v_batch_loss.item()\n",
    "\n",
    "              v_pred_max = torch.argmax(labels_pred, 1)\n",
    "              batch_acc = torch.sum(v_pred_max == labels)\n",
    "              val_acc = val_acc + batch_acc.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(val_acc / (BATCH_SIZE * len(val_loader)))\n",
    "      print(\"Epoch: {}, val loss: {:.4f}, val acc: {:.4f}, train loss: {:.4f}, train acc: {:.4f},\\n\".format(e, val_losses[e], val_accuracies[e], train_losses[e], train_accuracies[e]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CatBreedCNN.train()\n",
    "torch.save(CatBreedCNN.state_dict(), SAVE_TO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader) :\n",
    "  loss = nn.CrossEntropyLoss()\n",
    "  model.eval()\n",
    "  test_loss = 0.0\n",
    "  test_acc = 0.0\n",
    "  with torch.no_grad():\n",
    "    for (batch, labels) in loader:\n",
    "      batch, labels = batch.to(DEVICE), labels.to(DEVICE)\n",
    "      y_batch_pred = model(batch)\n",
    "      batch_loss = loss(y_batch_pred, labels)\n",
    "      test_loss = test_loss + batch_loss.item()\n",
    "\n",
    "      pred_max = torch.argmax(y_batch_pred, 1)\n",
    "      batch_acc = torch.sum(pred_max == labels)\n",
    "      test_acc = test_acc + batch_acc.item()\n",
    "    test_loss = test_loss / len(loader)\n",
    "    test_acc = test_acc / (BATCH_SIZE * len(loader))\n",
    "    return test_loss, test_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}